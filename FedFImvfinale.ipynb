{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUSqfp9Mh7oA"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Representation of trajectory and corresponding dataset in our work\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Itemset():\n",
        "    def __init__(self,data):\n",
        "        self.data = set(data)\n",
        "        self.data_length = len(data)\n",
        "\n",
        "    def checkSub(self,query):\n",
        "        for i in query:\n",
        "            if i not in self.data:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __eq__(self,other):\n",
        "        comp = (self.id() == other.id())\n",
        "        return comp\n",
        "\n",
        "    def id(self):\n",
        "        return tuple(list(self.data).sort())\n",
        "\n",
        "    #def get_line(self,index):\n",
        "        #return self.data[index]\n",
        "\n",
        "\n",
        "class ItemClient():\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "\n",
        "    def add_line(self, line):\n",
        "        self.data.append(line)\n",
        "\n",
        "    def get_line(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def get_line_num(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_line(index)\n",
        "\n",
        "\n",
        "class ItemDataSet():\n",
        "    def __init__(self):\n",
        "        self.record = []\n",
        "\n",
        "    def add_line(self, line):\n",
        "        self.record.append(line)\n",
        "\n",
        "    def get_line(self, index):\n",
        "        return self.record[index]\n",
        "\n",
        "    def get_line_num(self):\n",
        "        return len(self.record)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_line(index)\n",
        "\n",
        "    # The purpose of this function is to return the idx-th client\n",
        "    def get_trajectory(self, idx):\n",
        "        return self.record[idx]\n",
        "\n",
        "    def show_dataset(self, filename):\n",
        "        f = open(filename, 'a')\n",
        "        for i in self.record:\n",
        "            for j in i.data:\n",
        "                f.write(\"{}\".format(j))\n",
        "        #f.write(\"{}\".format(self.points))\n",
        "        # print(\"len(self.record):\", len(self.record))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwL2xc42jEy9",
        "outputId": "bc828174-0dca-46d7-d3e4-3a959df9f9ee"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import re\n",
        "\n",
        "# Convert a txt file to a pickle file\n",
        "\n",
        "data = ItemDataSet()\n",
        "num = 0\n",
        "# FedFIM can be used in the case where the client has multiple transactions.\n",
        "# For convenience reasons, in the experiments, we assume that each client has 7 transactions.\n",
        "trans_num = 7\n",
        "t = 0\n",
        "with open(\"T10I4D100K.txt\", \"r\") as f:\n",
        "\n",
        "    C = ItemClient()\n",
        "    for line in f:\n",
        "        num = num + 1\n",
        "        l1 = []\n",
        "        s = re.findall(r'\\d+', line)\n",
        "        for i in s:\n",
        "            l1.append(int(i))\n",
        "\n",
        "        x = Itemset(l1)\n",
        "        C.add_line(x)\n",
        "        if(num % trans_num == 0):\n",
        "            t = t + 1\n",
        "            data.add_line(C)\n",
        "            C=ItemClient()\n",
        "\n",
        "    if(C.data != []):\n",
        "         data.add_line(C)\n",
        "\n",
        "pickle.dump(data,open('connect.pickle','wb'))\n",
        "\n",
        "print(\"The total number of transactions in the dataset is: \",num)\n",
        "print(\"Num of clients: \",t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1HZjQUOiHn7",
        "outputId": "cdeebb01-55a3-470c-e803-b78793deec04"
      },
      "outputs": [],
      "source": [
        "pip install bitarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30-yjsxydpoJ",
        "outputId": "e2b76f20-00fb-43ef-8bc5-355f22afffe4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import bitarray\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import re  # ajoutÃ©\n",
        "\n",
        "# --- Classes et fonctions principales (simplifiÃ©es) ---\n",
        "\n",
        "class BMCTreeNode:\n",
        "    def __init__(self, item, count, bitmap_code):\n",
        "        self.item = item\n",
        "        self.count = count\n",
        "        self.bitmap_code = bitmap_code\n",
        "        self.children = dict()\n",
        "    def get_child_registering_item(self, item):\n",
        "        return self.children.get(item)\n",
        "    def add_child(self, child):\n",
        "        self.children[child.item] = child\n",
        "\n",
        "def clean_BMC_tree(root):\n",
        "    for _, child in root.children.items():\n",
        "        clean_BMC_tree(child)\n",
        "    del root.item\n",
        "    del root.children\n",
        "\n",
        "class FrequentItemsetTreeNode:\n",
        "    def __init__(self):\n",
        "        self.item = None\n",
        "        self.count = 0\n",
        "        self.children = []\n",
        "        self.NegNodeSet = []\n",
        "\n",
        "class NegFIN:\n",
        "    def __init__(self, args, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.min_support = args.k\n",
        "        self.client_num = dataset.get_line_num()\n",
        "        self.num_of_transactions = 0\n",
        "        self.num_participants = args.num_participants\n",
        "        self.output_file = \"outputNegFIN.txt\"\n",
        "        self.F1 = None\n",
        "        self.item_to_NodeSet = None\n",
        "        self.writer = None\n",
        "        self.num_of_frequent_itemsets = 0\n",
        "        self.execution_time = None\n",
        "        self.eta = 1 / (1 + math.exp(args.epsilon))\n",
        "        self.xi = args.xi\n",
        "        self.FPs = []\n",
        "\n",
        "    def runAlgorithm(self):\n",
        "        start_timestamp = datetime.now()\n",
        "        self.__generate_NodeSets_of_1_itemsets()\n",
        "        root = self.__create_root_of_frequent_itemset_tree()\n",
        "        self.writer = open(self.output_file, 'w')\n",
        "        itemset_buffer = [None] * len(self.F1)\n",
        "        itemset_length = 0\n",
        "        FIS_parent_buffer = [None] * len(self.F1)\n",
        "        FIS_parent_length = 0\n",
        "        num_of_children = len(root.children)\n",
        "        for _ in range(num_of_children):\n",
        "            child = root.children[0]\n",
        "            itemset_buffer[itemset_length] = child.item\n",
        "            del root.children[0]\n",
        "            self.__construct_frequent_itemset_tree(child, itemset_buffer, itemset_length + 1, root.children,\n",
        "                                                   FIS_parent_buffer, FIS_parent_length)\n",
        "        self.writer.close()\n",
        "        end_timestamp = datetime.now()\n",
        "        self.execution_time = (end_timestamp - start_timestamp).total_seconds() * 1000\n",
        "\n",
        "    def __generate_NodeSets_of_1_itemsets(self):\n",
        "        clientresponse = []\n",
        "        item_name_to_count = {}\n",
        "\n",
        "        if self.num_participants > self.client_num:\n",
        "            self.num_participants = self.client_num\n",
        "\n",
        "        RID = random.sample(range(0, self.client_num), self.num_participants)\n",
        "\n",
        "        num_r = -1\n",
        "        for cid in range(len(RID)):\n",
        "            for j in range(len(self.dataset.record[RID[cid]].data)):\n",
        "                clientresponse.append([])\n",
        "                num_r += 1\n",
        "                fx = list(self.dataset.record[RID[cid]].data[j].data)\n",
        "                for t in range(len(fx)):\n",
        "                    draw = np.random.random_sample()\n",
        "                    response = 1\n",
        "                    if draw < self.eta:\n",
        "                        response = 1 - response\n",
        "                    if response == 1:\n",
        "                        clientresponse[num_r].append(fx[t])\n",
        "                        item_name_to_count[fx[t]] = item_name_to_count.get(fx[t], 0) + 1\n",
        "\n",
        "        self.num_of_transactions = num_r + 1\n",
        "        item_name_to_count.pop('', None)\n",
        "        self.min_count = math.ceil(self.num_of_transactions * self.min_support)\n",
        "        upper_thres = self.min_support * (1 - self.eta) + (1 - self.min_support) * self.eta + math.sqrt(\n",
        "            -math.log(self.xi) / (2 * self.num_of_transactions))\n",
        "        self.F1 = [{'name': item_name, 'count': item_count} for item_name, item_count in item_name_to_count.items()\n",
        "                   if upper_thres * self.num_of_transactions <= item_count]\n",
        "        self.F1.sort(key=lambda item: item['count'])\n",
        "        self.F1 = tuple(self.F1)\n",
        "        item_name_to_item_index = {item['name']: idx for idx, item in enumerate(self.F1)}\n",
        "        self.item_to_NodeSet = {idx: [] for idx in item_name_to_item_index.values()}\n",
        "\n",
        "        bmc_tree_root = BMCTreeNode(item=None, count=None, bitmap_code=bitarray.bitarray([False] * len(self.F1)))\n",
        "        for cid in range(self.num_of_transactions):\n",
        "            transaction = [item_name_to_item_index[item] for item in clientresponse[cid] if item in item_name_to_item_index]\n",
        "            transaction.sort(reverse=True)\n",
        "            cur_root = bmc_tree_root\n",
        "            for item in transaction:\n",
        "                N = cur_root.get_child_registering_item(item)\n",
        "                if N is None:\n",
        "                    bitmap_code = cur_root.bitmap_code.copy()\n",
        "                    bitmap_code[item] = True\n",
        "                    N = BMCTreeNode(item=item, count=0, bitmap_code=bitmap_code)\n",
        "                    cur_root.add_child(N)\n",
        "                    self.item_to_NodeSet[item].append(N)\n",
        "                N.count += 1\n",
        "                cur_root = N\n",
        "        clean_BMC_tree(bmc_tree_root)\n",
        "\n",
        "    def __create_root_of_frequent_itemset_tree(self):\n",
        "        root = FrequentItemsetTreeNode()\n",
        "        for item in range(len(self.F1)):\n",
        "            child = FrequentItemsetTreeNode()\n",
        "            child.item = item\n",
        "            child.count = self.F1[item]['count']\n",
        "            child.NegNodeSet = self.item_to_NodeSet[item]\n",
        "            root.children.append(child)\n",
        "        return root\n",
        "\n",
        "    def __write_itemsets_to_file(self, N, itemset_buffer, N_itemset_length, FIS_parent_buffer, FIS_parent_length):\n",
        "        file_buffer = []\n",
        "        self.num_of_frequent_itemsets += 1\n",
        "        itemset_string = [self.F1[itemset_buffer[i]]['name'] for i in range(N_itemset_length)]\n",
        "        t = {'data': itemset_string.copy(), 'support': N.count / self.num_of_transactions}\n",
        "        self.FPs.append(t)\n",
        "        itemset_string.append(f'#SUP: {N.count}\\n')\n",
        "        file_buffer.append(' '.join(str(x) for x in itemset_string))\n",
        "        if FIS_parent_length > 0:\n",
        "            max_comb = 1 << FIS_parent_length\n",
        "            for i in range(1, max_comb):\n",
        "                itemset_string = [self.F1[itemset_buffer[i]]['name'] for i in range(N_itemset_length)]\n",
        "                subset_string = [self.F1[FIS_parent_buffer[j]]['name'] for j in range(FIS_parent_length) if (i & (1 << j)) > 0]\n",
        "                itemset_string.extend(subset_string)\n",
        "                t = {'data': itemset_string.copy(), 'support': N.count / self.num_of_transactions}\n",
        "                self.FPs.append(t)\n",
        "                itemset_string.append(f'#SUP: {N.count}\\n')\n",
        "                line = ' '.join(str(x) for x in itemset_string)\n",
        "                file_buffer.append(line)\n",
        "                self.num_of_frequent_itemsets += 1\n",
        "        self.writer.writelines(file_buffer)\n",
        "\n",
        "    def __construct_frequent_itemset_tree(self, N, itemset_buffer, N_itemset_length, N_right_siblings, FIS_parent_buffer, FIS_parent_length):\n",
        "        for sibling in N_right_siblings:\n",
        "            child = FrequentItemsetTreeNode()\n",
        "            sum_of_NegNodeSets_counts = 0\n",
        "            if N_itemset_length == 1:\n",
        "                for ni in N.NegNodeSet:\n",
        "                    if not ni.bitmap_code[sibling.item]:\n",
        "                        child.NegNodeSet.append(ni)\n",
        "                        sum_of_NegNodeSets_counts += ni.count\n",
        "            else:\n",
        "                for nj in sibling.NegNodeSet:\n",
        "                    if nj.bitmap_code[N.item]:\n",
        "                        child.NegNodeSet.append(nj)\n",
        "                        sum_of_NegNodeSets_counts += nj.count\n",
        "            child.count = N.count - sum_of_NegNodeSets_counts\n",
        "            if self.min_count <= child.count:\n",
        "                if N.count == child.count:\n",
        "                    FIS_parent_buffer[FIS_parent_length] = sibling.item\n",
        "                    FIS_parent_length += 1\n",
        "                else:\n",
        "                    child.item = sibling.item\n",
        "                    N.children.append(child)\n",
        "        self.__write_itemsets_to_file(N, itemset_buffer, N_itemset_length, FIS_parent_buffer, FIS_parent_length)\n",
        "        number_of_children = len(N.children)\n",
        "        for _ in range(number_of_children):\n",
        "            child = N.children[0]\n",
        "            itemset_buffer[N_itemset_length] = child.item\n",
        "            del N.children[0]\n",
        "            self.__construct_frequent_itemset_tree(child, itemset_buffer, N_itemset_length + 1, N.children, FIS_parent_buffer, FIS_parent_length)\n",
        "\n",
        "class FLFIM_Model:\n",
        "    def __init__(self, args, dataset):\n",
        "        self.args = args\n",
        "        self.dataset = dataset\n",
        "    def run(self):\n",
        "        algorithm = NegFIN(self.args, self.dataset)\n",
        "        algorithm.runAlgorithm()\n",
        "        return algorithm.FPs\n",
        "\n",
        "class Args:\n",
        "    def __init__(self, xi, k, num_participants, epsilon, dataset):\n",
        "        self.xi = xi\n",
        "        self.k = k\n",
        "        self.num_participants = num_participants\n",
        "        self.epsilon = epsilon\n",
        "        self.dataset = dataset  # ajout de l'attribut\n",
        "\n",
        "# --- Fonction pour calculer F1 score ---\n",
        "\n",
        "def compute_f1_score(predicted, truth):\n",
        "    # Convertir les listes d'itemsets en sets de tuples triÃ©s pour comparer sans ordre\n",
        "    predicted_set = set(tuple(sorted(itemset)) for itemset in predicted)\n",
        "    truth_set = set(tuple(sorted(itemset)) for itemset in truth)\n",
        "\n",
        "    tp = len(predicted_set.intersection(truth_set))\n",
        "    precision = tp / len(predicted_set) if predicted_set else 0\n",
        "    recall = tp / len(truth_set) if truth_set else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "    print(f\"TP: {tp}, FP: {len(predicted_set) - tp}, FN: {len(truth_set) - tp}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "def SupportCountPickleName(args):\n",
        "    sanitized_dataset = re.sub(r'[^a-zA-Z0-9_-]', '', str(args.dataset))\n",
        "    name = \"supportcount_\" + sanitized_dataset + '.txt'\n",
        "    return name\n",
        "\n",
        "def GetGroundTruth(args):\n",
        "    scName = SupportCountPickleName(args)\n",
        "    if os.path.isfile(scName):\n",
        "        with open(scName,'rb') as fp:\n",
        "            sc_rec = pickle.load(fp)\n",
        "            ground_truth = [i['data'] for i in sc_rec]\n",
        "            return ground_truth\n",
        "    else:\n",
        "        print(f\"Ground truth file '{scName}' not found.\")\n",
        "        return []\n",
        "\n",
        "class groundTruth():\n",
        "    def __init__(self,args,dataset):\n",
        "        self.args = args\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def run(self):\n",
        "        algorithm = NegFIN(self.args,self.dataset)\n",
        "        algorithm.runAlgorithm()\n",
        "        return algorithm.FPs\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Chargement du dataset\n",
        "    print(\"Chargement du dataset retail.pickle...\")\n",
        "    with open('connect.pickle', 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "\n",
        "    # Listes des paramÃ¨tres Ã  tester\n",
        "    epsilons = [1.0, 3.0, 5.0, 7.0, 9.0]\n",
        "    kis = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for epsilon in epsilons:\n",
        "        for ki in kis:\n",
        "            print(f\"\\nTest avec epsilon={epsilon}, three={ki}\")\n",
        "\n",
        "            args = Args(xi=0.2, k=ki, num_participants=250, epsilon=epsilon, dataset=dataset)\n",
        "\n",
        "            # ExÃ©cution modÃ¨le bruyant\n",
        "            model_noisy = FLFIM_Model(args, dataset)\n",
        "            noisy_results = model_noisy.run()\n",
        "            predicted_itemsets = [fp['data'] for fp in noisy_results]\n",
        "\n",
        "            # ExÃ©cution modÃ¨le vÃ©ritÃ© terrain\n",
        "            truth_model = groundTruth(args, dataset)\n",
        "            truth_results = truth_model.run()\n",
        "            true_itemsets = [fp['data'] for fp in truth_results]\n",
        "\n",
        "            # Calcul scores\n",
        "            precision, recall, f1 = compute_f1_score(predicted_itemsets, true_itemsets)\n",
        "\n",
        "            results.append({\n",
        "                'dataset_name': \"chess\",\n",
        "                'epsilon': epsilon,\n",
        "                'min_sup': ki,\n",
        "                'f1_score': f1\n",
        "            })\n",
        "\n",
        "    # Sauvegarde dans un fichier JSON\n",
        "    with open('FedFimconnect.json', 'w') as json_file:\n",
        "        json.dump(results, json_file, indent=4)\n",
        "\n",
        "    print(\"\\nTests terminÃ©s. RÃ©sultats sauvegardÃ©s dans FedFimskin.json\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
